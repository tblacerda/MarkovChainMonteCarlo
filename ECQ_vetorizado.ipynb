{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\F8058552\\AppData\\Local\\anaconda3\\envs\\clusters\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install gxx`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas do dataset: Index(['ANF', 'MUNICIPIO', 'ENDERECO_ID', 'TESTES_ECQ', 'TESTES_ECQ_OK'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "\n",
    "# Carregar os dados\n",
    "data = pd.read_excel('ECQ_FEV25.xlsx')\n",
    "\n",
    "# Verificar as colunas disponíveis\n",
    "print(\"Colunas do dataset:\", data.columns)\n",
    "\n",
    "# Converter as variáveis categóricas para códigos numéricos\n",
    "data['anf_idx'] = data['ANF'].astype('category').cat.codes\n",
    "data['mun_idx'] = data['MUNICIPIO'].astype('category').cat.codes\n",
    "\n",
    "# Criar um mapeamento que relacione cada município ao seu ANF\n",
    "# Assumindo que cada município pertença a um único ANF\n",
    "mun_to_anf = data.drop_duplicates('mun_idx').sort_values('mun_idx')['anf_idx'].values\n",
    "\n",
    "# Arrays com os dados observados\n",
    "n_tests = data['TESTES_ECQ'].values\n",
    "n_success = data['TESTES_ECQ_OK'].values\n",
    "\n",
    "# Número total de observações (sites) e de níveis hierárquicos\n",
    "n_sites = data.shape[0]\n",
    "n_anfs = data['anf_idx'].nunique()\n",
    "n_mun = data['mun_idx'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu_anf, sigma_anf, phi_mun, mu_mun, phi_site, theta_site]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7b697367fd47a4887316e44b00f581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Modelo vetorizado\n",
    "with pm.Model() as modelo_vectorizado:\n",
    "    # Dados observados via pm.Data para permitir atualizações sem recompilar o modelo\n",
    "    n_tests_data = pm.Data(\"n_tests\", n_tests)\n",
    "    n_success_data = pm.Data(\"n_success\", n_success)\n",
    "    \n",
    "    # Nível 3: Parâmetros para as ANFs\n",
    "    mu_anf = pm.Beta(\"mu_anf\", alpha=2, beta=2, shape=n_anfs)\n",
    "    sigma_anf = pm.HalfNormal(\"sigma_anf\", sigma=0.1, shape=n_anfs)\n",
    "    \n",
    "    # Nível 2: Parâmetros para os municípios\n",
    "    # Aqui, definimos um phi individual para cada município para capturar a concentração\n",
    "    phi_mun = pm.HalfNormal(\"phi_mun\", sigma=10, shape=n_mun)\n",
    "    \n",
    "    # Parâmetros para os municípios, condicionado ao nível de ANF\n",
    "    mu_mun = pm.Beta(\n",
    "        \"mu_mun\",\n",
    "        alpha=mu_anf[mun_to_anf] * phi_mun,\n",
    "        beta=(1 - mu_anf[mun_to_anf]) * phi_mun,\n",
    "        shape=n_mun\n",
    "    )\n",
    "    \n",
    "    # Nível 1: Parâmetro para os sites (observações individuais)\n",
    "    phi_site = pm.HalfNormal(\"phi_site\", sigma=10)\n",
    "    \n",
    "    # Cada site é associado ao seu município via data[\"mun_idx\"]\n",
    "    theta_site = pm.Beta(\n",
    "        \"theta_site\",\n",
    "        alpha=mu_mun[data['mun_idx'].values] * phi_site,\n",
    "        beta=(1 - mu_mun[data['mun_idx'].values]) * phi_site,\n",
    "        shape=n_sites\n",
    "    )\n",
    "    \n",
    "    # Likelihood: Modelo Binomial para cada observação\n",
    "    obs = pm.Binomial(\n",
    "        \"obs\",\n",
    "        n=n_tests_data,\n",
    "        p=theta_site,\n",
    "        observed=n_success_data\n",
    "    )\n",
    "    \n",
    "    # Amostragem\n",
    "    trace = pm.sample(4000, tune=100, chains=4, target_accept=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Converter o trace para InferenceData\u001b[39;00m\n\u001b[0;32m      2\u001b[0m idata \u001b[38;5;241m=\u001b[39m az\u001b[38;5;241m.\u001b[39mInferenceData(\n\u001b[1;32m----> 3\u001b[0m     posterior\u001b[38;5;241m=\u001b[39m\u001b[43mtrace\u001b[49m\u001b[38;5;241m.\u001b[39mposterior,\n\u001b[0;32m      4\u001b[0m     sample_stats\u001b[38;5;241m=\u001b[39mtrace\u001b[38;5;241m.\u001b[39msample_stats,\n\u001b[0;32m      5\u001b[0m     observed_data\u001b[38;5;241m=\u001b[39mtrace\u001b[38;5;241m.\u001b[39mobserved_data\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Salvar em NetCDF (garanta que o caminho está correto)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m idata\u001b[38;5;241m.\u001b[39mto_netcdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_ECQ_.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trace' is not defined"
     ]
    }
   ],
   "source": [
    "# Converter o trace para InferenceData\n",
    "idata = az.InferenceData(\n",
    "    posterior=trace.posterior,\n",
    "    sample_stats=trace.sample_stats,\n",
    "    observed_data=trace.observed_data\n",
    ")\n",
    "\n",
    "# Salvar em NetCDF (garanta que o caminho está correto)\n",
    "idata.to_netcdf(\"trace_ECQ_.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leia o modelo em trace_ECC.nc\n",
    "idata = az.from_netcdf(\"trace_ECQ_.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piores municípios por ANF:\n",
      "          mun_idx   mean\n",
      "anf                     \n",
      "0   1200     1200  0.408\n",
      "1   523       523  0.057\n",
      "2   632       632  0.064\n",
      "3   1207     1207  0.037\n",
      "4   752       752  0.061\n",
      "5   318       318  0.097\n",
      "6   610       610  0.081\n",
      "7   842       842  0.063\n",
      "8   1340     1340  0.073\n",
      "9   1215     1215  0.122\n",
      "10  493       493  0.078\n",
      "11  1038     1038  0.059\n",
      "12  434       434  0.035\n",
      "13  288       288  0.077\n",
      "14  1333     1333  0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F8058552\\AppData\\Local\\Temp\\ipykernel_18312\\2376533117.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  piores_municipios = summary_mun.groupby('anf').apply(lambda df: df.nsmallest(1, 'mean'))\n"
     ]
    }
   ],
   "source": [
    "# 2. De cada ANF, quais os piores municípios?\n",
    "# Os municípios estão modelados via o parâmetro mu_mun, que depende do ANF\n",
    "# a que pertencem. Para cada ANF, você pode:\n",
    "# Obter o resumo dos parâmetros dos municípios.\n",
    "#\n",
    "# Usar o mapeamento (vetor mun_to_anf) para agrupar os municípios por ANF.\n",
    "# Dentro de cada grupo, identificar os municípios com menores médias.\n",
    "# Obter o resumo do parâmetro mu_mun\n",
    "summary_mun = az.summary(idata, var_names=[\"mu_mun\"])\n",
    "summary_mun = summary_mun.reset_index().rename(columns={'index': 'mun_idx'})\n",
    "\n",
    "# Converter o mun_idx para inteiro e associar ao ANF correspondente\n",
    "summary_mun['mun_idx'] = summary_mun['mun_idx'].str.extract('(\\d+)').astype(int)\n",
    "summary_mun['anf'] = [mun_to_anf[i] for i in summary_mun['mun_idx']]\n",
    "\n",
    "# Agrupar por ANF e identificar o município com menor média\n",
    "piores_municipios = summary_mun.groupby('anf').apply(lambda df: df.nsmallest(1, 'mean'))\n",
    "print(\"Piores municípios por ANF:\")\n",
    "print(piores_municipios[['mun_idx', 'mean']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo por ANF:\n",
      "   ANF_idx  ANF   mean  hdi_2.5%  hdi_97.5%\n",
      "0        0   71  0.597     0.506      0.693\n",
      "1        1   73  0.416     0.355      0.481\n",
      "2        2   74  0.361     0.285      0.440\n",
      "3        3   75  0.406     0.361      0.454\n",
      "4        4   77  0.358     0.300      0.416\n",
      "\n",
      "Resumo por Município:\n",
      "   mun_idx     MUNICIPIO  ANF_idx  ANF   mean  hdi_2.5%  hdi_97.5%\n",
      "0        0       ABAIARA       13   88  0.134     0.000      0.382\n",
      "1        1        ABAIRA        4   77  0.228     0.000      0.488\n",
      "2        2         ABARE        3   75  0.416     0.116      0.689\n",
      "3        3  ABREU E LIMA        6   81  0.654     0.502      0.814\n",
      "4        4     ACAJUTIBA        3   75  0.271     0.041      0.495\n",
      "\n",
      "Resumo por Site:\n"
     ]
    }
   ],
   "source": [
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supondo que 'data' é o dataframe original e 'mun_to_anf' é o array que mapeia cada município ao seu ANF.\n",
    "# Também assumindo que as colunas originais 'ANF', 'MUNICIPIO' e 'ENDERECO_ID' existem no dataframe.\n",
    "\n",
    "# Crie os mapeamentos dos índices para os nomes originais\n",
    "anf_categories = data['ANF'].astype('category').cat.categories\n",
    "anf_mapping = dict(enumerate(anf_categories))  # chave: código numérico; valor: nome do ANF\n",
    "\n",
    "mun_categories = data['MUNICIPIO'].astype('category').cat.categories\n",
    "mun_mapping = dict(enumerate(mun_categories))  # chave: código numérico; valor: nome do município\n",
    "\n",
    "# --- 1. Resumo dos parâmetros dos ANFs (mu_anf) ---\n",
    "summary_anf = az.summary(idata, var_names=[\"mu_anf\"], hdi_prob=0.95).reset_index()\n",
    "# O índice vem como \"mu_anf[0]\", \"mu_anf[1]\", etc. Extraímos o número\n",
    "summary_anf[\"ANF_idx\"] = summary_anf[\"index\"].str.extract(r'\\[(\\d+)\\]').astype(int)\n",
    "# Mapeia para o nome real\n",
    "summary_anf[\"ANF\"] = summary_anf[\"ANF_idx\"].map(anf_mapping)\n",
    "# Reorganiza as colunas\n",
    "df_anf = summary_anf[[\"ANF_idx\", \"ANF\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\"]]\n",
    "print(\"Resumo por ANF:\")\n",
    "print(df_anf.head())\n",
    "\n",
    "# --- 2. Resumo dos parâmetros dos Municípios (mu_mun) ---\n",
    "summary_mun = az.summary(idata, var_names=[\"mu_mun\"], hdi_prob=0.95).reset_index()\n",
    "summary_mun[\"mun_idx\"] = summary_mun[\"index\"].str.extract(r'\\[(\\d+)\\]').astype(int)\n",
    "# Mapeia para o nome do município\n",
    "summary_mun[\"MUNICIPIO\"] = summary_mun[\"mun_idx\"].map(mun_mapping)\n",
    "# Adiciona o índice do ANF usando o vetor mun_to_anf\n",
    "summary_mun[\"ANF_idx\"] = summary_mun[\"mun_idx\"].apply(lambda i: mun_to_anf[i])\n",
    "summary_mun[\"ANF\"] = summary_mun[\"ANF_idx\"].map(anf_mapping)\n",
    "# Organiza as colunas desejadas\n",
    "df_mun = summary_mun[[\"mun_idx\", \"MUNICIPIO\", \"ANF_idx\", \"ANF\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\"]]\n",
    "print(\"\\nResumo por Município:\")\n",
    "print(df_mun.head())\n",
    "\n",
    "# --- 3. Resumo dos parâmetros dos Sites (theta_site) ---\n",
    "summary_site = az.summary(idata, var_names=[\"theta_site\"], hdi_prob=0.95).reset_index()\n",
    "summary_site[\"site_idx\"] = summary_site[\"index\"].str.extract(r'\\[(\\d+)\\]').astype(int)\n",
    "\n",
    "# Crie um dataframe dos metadados dos sites a partir do original.\n",
    "# Supondo que cada linha do 'data' corresponde a um site e que o índice do dataframe\n",
    "# corresponde ao site_idx usado no modelo.\n",
    "df_site_meta = data.reset_index().rename(columns={'index': 'site_idx'})\n",
    "# Se já não existir, garanta que as colunas \"mun_idx\" e \"ANF\" estejam presentes.\n",
    "# Se necessário, adicione-as:\n",
    "if \"mun_idx\" not in df_site_meta.columns:\n",
    "    df_site_meta[\"mun_idx\"] = data['MUNICIPIO'].astype('category').cat.codes\n",
    "if \"ANF\" not in df_site_meta.columns:\n",
    "    df_site_meta[\"ANF\"] = data['ANF']\n",
    "\n",
    "# Acrescente o nome do município e o nome do ANF, usando os mapeamentos:\n",
    "df_site_meta[\"MUNICIPIO\"] = df_site_meta[\"mun_idx\"].map(mun_mapping)\n",
    "# Também é possível adicionar o código do ANF a partir do vetor mun_to_anf:\n",
    "df_site_meta[\"ANF_idx\"] = df_site_meta[\"mun_idx\"].apply(lambda i: mun_to_anf[i])\n",
    "df_site_meta[\"ANF\"] = df_site_meta[\"ANF_idx\"].map(anf_mapping)\n",
    "\n",
    "# Agora, combine o resumo de theta_site com os metadados do site.\n",
    "df_site = pd.merge(summary_site, df_site_meta[[\"site_idx\", \"ENDERECO_ID\", \"MUNICIPIO\", \"ANF\"]],\n",
    "                   on=\"site_idx\", how=\"left\")\n",
    "# Organize as colunas relevantes\n",
    "df_site = df_site[[\"site_idx\", \"ENDERECO_ID\", \"MUNICIPIO\", \"ANF\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\"]]\n",
    "print(\"\\nResumo por Site:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salve o dataframe com o resumo dos parâmetros dos municípios e dos sites em sheets diferentes\n",
    "with pd.ExcelWriter(\"resumo_municipios_sites.xlsx\") as writer:\n",
    "    df_mun.to_excel(writer, sheet_name=\"Municipios\", index=False)\n",
    "    df_site.to_excel(writer, sheet_name=\"Sites\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de QUEIMADAS: 54.7%\n",
      "ENDERECO_IDs significativamente abaixo da média do município:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ENDERECO_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hdi_2.5%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hdi_97.5%",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "309ea1af-1503-42e0-8d15-747e43042dc4",
       "rows": [
        [
         "2685",
         "PBQUS_0001",
         "0.362",
         "0.199",
         "0.538"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENDERECO_ID</th>\n",
       "      <th>mean</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>PBQUS_0001</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ENDERECO_ID   mean  hdi_2.5%  hdi_97.5%\n",
       "2685  PBQUS_0001  0.362     0.199      0.538"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supondo que o município \"João Pessoa\" esteja em df_mun e df_site\n",
    "mun_nome = \"JOAO PESSOA\"\n",
    "\n",
    "# Média e HDI do município (usando df_mun)\n",
    "mun_info = df_mun[df_mun[\"MUNICIPIO\"] == mun_nome].iloc[0]\n",
    "mun_media = mun_info[\"mean\"]\n",
    "\n",
    "# Filtrar os sites desse município\n",
    "sites_joao = df_site[df_site[\"MUNICIPIO\"] == mun_nome]\n",
    "\n",
    "# Selecionar os sites cujo limite superior do HDI (por exemplo, \"hdi_97.0%\") esteja abaixo da média do município.\n",
    "mun_media_percentual = round(mun_media * 100, 2)\n",
    "print(f\"Média de {mun_nome}: {mun_media_percentual}%\")\n",
    "sites_abaixo = sites_joao[sites_joao[\"hdi_97.5%\"] < mun_media]\n",
    "\n",
    "print(\"ENDERECO_IDs significativamente abaixo da média do município:\")\n",
    "sites_abaixo = sites_abaixo.sort_values(\"hdi_97.5%\", ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Se preciso elevar o ECQ do município em 2 pontos percentuais, quais ENDERECO_IDs preciso atualizar?\n",
    "----\n",
    "Suponha que a meta seja aumentar a média do município em 0,02 (2 pontos percentuais). O procedimento pode ser:\n",
    "\n",
    "Definir o novo limiar desejado:\n",
    "limite_novo = mun_media + 0.02\n",
    "\n",
    "\n",
    "Selecionar os sites do município cujo limite superior do HDI esteja abaixo desse novo alvo, ou seja, aqueles que provavelmente não atingem o desempenho desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novo alvo para QUEIMADAS: 0.8\n",
      "ENDERECO_IDs para intervenção (ECQ abaixo do novo alvo):\n"
     ]
    }
   ],
   "source": [
    "# Insira o nome do município alvo\n",
    "MUNICIPIO_ALVO = \"QUEIMADAS\"\n",
    "# Meta de aumento em 2 pontos percentuais\n",
    "alvo = 0.8\n",
    "\n",
    "# Calcule o impacto para cada site: quanto maior, maior a prioridade\n",
    "df_site[\"impacto\"] = df_site[\"TESTES_ECQ\"] * (alvo - df_site[\"mean\"])\n",
    "\n",
    "# Ordene os sites pelo indicador de impacto (maior valor tem maior potencial de melhoria)\n",
    "df_site_impacto = df_site.sort_values(\"impacto\", ascending=False)\n",
    "print(\"Sites com maior potencial de impacto:\")\n",
    "print(df_site_impacto[[\"ENDERECO_ID\", \"mean\", \"TESTES_ECQ\", \"impacto\"]].head())\n",
    "\n",
    "\n",
    "# Média e HDI do município (usando df_mun)\n",
    "mun_info = df_mun[df_mun[\"MUNICIPIO\"] == MUNICIPIO_ALVO].iloc[0]\n",
    "mun_media = mun_info[\"mean\"]\n",
    "\n",
    "\n",
    "print(f\"Novo alvo para {MUNICIPIO_ALVO}: {alvo}\")\n",
    "\n",
    "# Filtrar os sites do município alvo\n",
    "sites_alvo = df_site[df_site[\"MUNICIPIO\"] == MUNICIPIO_ALVO]\n",
    "\n",
    "# Selecionar sites cujo limite superior do HDI esteja abaixo do alvo\n",
    "sites_para_atuar = sites_alvo[sites_alvo[\"hdi_97.5%\"] < alvo]\n",
    "\n",
    "print(\"ENDERECO_IDs para intervenção (ECQ abaixo do novo alvo):\")\n",
    "sites_para_atuar[[\"ENDERECO_ID\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\"]].sort_values(\"hdi_97.5%\", ascending=True)\n",
    "\n",
    "# Calcule o impacto para cada site: quanto maior, maior a prioridade\n",
    "df_ECQ_TESTES = data[['ENDERECO_ID', 'TESTES_ECQ']]\n",
    "# merge df_\n",
    "df_site = pd.merge(df_site, df_ECQ_TESTES, on='ENDERECO_ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo por ANF:\n",
      "   ANF_idx  ANF   mean  hdi_2.5%  hdi_97.5%\n",
      "0        0   71  0.597     0.506      0.693\n",
      "1        1   73  0.416     0.355      0.481\n",
      "2        2   74  0.361     0.285      0.440\n",
      "3        3   75  0.406     0.361      0.454\n",
      "4        4   77  0.358     0.300      0.416\n",
      "\n",
      "Resumo por Município:\n",
      "   mun_idx     MUNICIPIO  ANF_idx  ANF   mean  hdi_2.5%  hdi_97.5%\n",
      "0        0       ABAIARA       13   88  0.134     0.000      0.382\n",
      "1        1        ABAIRA        4   77  0.228     0.000      0.488\n",
      "2        2         ABARE        3   75  0.416     0.116      0.689\n",
      "3        3  ABREU E LIMA        6   81  0.654     0.502      0.814\n",
      "4        4     ACAJUTIBA        3   75  0.271     0.041      0.495\n",
      "\n",
      "Resultado Final:\n",
      "     MUNICIPIO  ANF ENDERECO_ID   mean  hdi_2.5%  hdi_97.5%  impacto\n",
      "0       ANADIA   82  ALAAD_0001  0.691     0.545      0.838    3.379\n",
      "1       ANADIA   82  ALAAD_0002  0.611     0.477      0.746    8.316\n",
      "2  AGUA BRANCA   82  ALABN_0001  0.590     0.399      0.791    3.570\n",
      "3  AGUA BRANCA   82  ALABN_0002  0.423     0.298      0.546   19.604\n",
      "4    ARAPIRACA   82  ALAIR_0001  0.785     0.709      0.859    1.650\n"
     ]
    }
   ],
   "source": [
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregue o modelo salvo\n",
    "idata = az.from_netcdf(\"trace_ECQ_.nc\")\n",
    "\n",
    "# --- 1. Resumo dos parâmetros dos ANFs (mu_anf) ---\n",
    "summary_anf = az.summary(idata, var_names=[\"mu_anf\"], hdi_prob=0.95).reset_index()\n",
    "# Extraia o índice numérico: \"mu_anf[0]\", \"mu_anf[1]\", etc.\n",
    "summary_anf[\"ANF_idx\"] = summary_anf[\"index\"].str.extract(r'\\[(\\d+)\\]').astype(int)\n",
    "# Mapeie para o nome real do ANF usando o mapeamento de categorias\n",
    "anf_categories = data['ANF'].astype('category').cat.categories\n",
    "anf_mapping = dict(enumerate(anf_categories))\n",
    "summary_anf[\"ANF\"] = summary_anf[\"ANF_idx\"].map(anf_mapping)\n",
    "# Organize as colunas desejadas\n",
    "df_anf = summary_anf[[\"ANF_idx\", \"ANF\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\"]]\n",
    "print(\"Resumo por ANF:\")\n",
    "print(df_anf.head())\n",
    "\n",
    "# --- 2. Resumo dos parâmetros dos Municípios (mu_mun) ---\n",
    "summary_mun = az.summary(idata, var_names=[\"mu_mun\"], hdi_prob=0.95).reset_index()\n",
    "summary_mun[\"mun_idx\"] = summary_mun[\"index\"].str.extract(r'\\[(\\d+)\\]').astype(int)\n",
    "# Mapeie para o nome do município usando os dados originais\n",
    "mun_categories = data['MUNICIPIO'].astype('category').cat.categories\n",
    "mun_mapping = dict(enumerate(mun_categories))\n",
    "summary_mun[\"MUNICIPIO\"] = summary_mun[\"mun_idx\"].map(mun_mapping)\n",
    "# Utilize o vetor mun_to_anf para associar o índice do ANF\n",
    "summary_mun[\"ANF_idx\"] = summary_mun[\"mun_idx\"].apply(lambda i: mun_to_anf[i])\n",
    "summary_mun[\"ANF\"] = summary_mun[\"ANF_idx\"].map(anf_mapping)\n",
    "# Organize as colunas desejadas\n",
    "df_mun = summary_mun[[\"mun_idx\", \"MUNICIPIO\", \"ANF_idx\", \"ANF\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\"]]\n",
    "print(\"\\nResumo por Município:\")\n",
    "print(df_mun.head())\n",
    "\n",
    "# --- 3. Resumo dos parâmetros dos Sites (theta_site) ---\n",
    "summary_site = az.summary(idata, var_names=[\"theta_site\"], hdi_prob=0.95).reset_index()\n",
    "summary_site[\"site_idx\"] = summary_site[\"index\"].str.extract(r'\\[(\\d+)\\]').astype(int)\n",
    "\n",
    "# Crie um DataFrame de metadados dos sites a partir do DataFrame original \"data\"\n",
    "df_site_meta = data.reset_index().rename(columns={'index': 'site_idx'})\n",
    "# Caso as colunas \"mun_idx\" e \"ANF\" não existam, gere-as:\n",
    "if \"mun_idx\" not in df_site_meta.columns:\n",
    "    df_site_meta[\"mun_idx\"] = data['MUNICIPIO'].astype('category').cat.codes\n",
    "df_site_meta[\"MUNICIPIO\"] = df_site_meta[\"mun_idx\"].map(mun_mapping)\n",
    "df_site_meta[\"ANF_idx\"] = df_site_meta[\"mun_idx\"].apply(lambda i: mun_to_anf[i])\n",
    "df_site_meta[\"ANF\"] = df_site_meta[\"ANF_idx\"].map(anf_mapping)\n",
    "\n",
    "# Combine o resumo de theta_site com os metadados\n",
    "df_site = pd.merge(summary_site, df_site_meta[[\"site_idx\", \"ENDERECO_ID\", \"MUNICIPIO\", \"ANF\"]], on=\"site_idx\", how=\"left\")\n",
    "df_site = df_site[[\"site_idx\", \"ENDERECO_ID\", \"MUNICIPIO\", \"ANF\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\"]]\n",
    "\n",
    "# --- 4. Mesclar o número de testes por site ---\n",
    "df_ECQ_TESTES = data[['ENDERECO_ID', 'TESTES_ECQ']]\n",
    "df_site = pd.merge(df_site, df_ECQ_TESTES, on='ENDERECO_ID', how=\"left\")\n",
    "\n",
    "# --- 5. Cálculo do Impacto ---\n",
    "# Defina o alvo de ECQ\n",
    "alvo = 0.8\n",
    "# Impacto = número de testes * (alvo - performance atual)\n",
    "df_site[\"impacto\"] = df_site[\"TESTES_ECQ\"] * (alvo - df_site[\"mean\"])\n",
    "\n",
    "# --- 6. Salvar o resultado final ---\n",
    "# O DataFrame final conterá: MUNICIPIO, ANF, ENDERECO_ID, mean, hdi_2.5%, hdi_97.5% e impacto\n",
    "df_resultado = df_site[[\"MUNICIPIO\", \"ANF\", \"ENDERECO_ID\", \"mean\", \"hdi_2.5%\", \"hdi_97.5%\", \"impacto\"]]\n",
    "\n",
    "# Exiba os primeiros registros do resultado\n",
    "print(\"\\nResultado Final:\")\n",
    "print(df_resultado.head())\n",
    "\n",
    "# Opcional: salvar em um arquivo Excel\n",
    "df_resultado.to_excel(\"resultado_impacto_sites.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Indicador de Impacto para Intervenção\n",
    "\n",
    "Se o objetivo é identificar onde os investimentos terão maior impacto para elevar o ECQ do município, você pode criar um indicador que combine o “déficit” de desempenho com o volume de testes. Por exemplo, se o alvo para o ECQ for \\( T \\) (por exemplo, 80% ou 0,80), defina para cada site:\n",
    "\n",
    "\\[\n",
    "\\text{Impacto}_i = \\text{TESTES}_i \\times (T - \\text{mean}_i)\n",
    "\\]\n",
    "\n",
    "Sites com maior valor desse indicador representam oportunidades onde há muito volume e, ao mesmo tempo, um déficit de desempenho relevante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "site_idx",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "ENDERECO_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MUNICIPIO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ANF",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hdi_2.5%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hdi_97.5%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TESTES_ECQ",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "impacto",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6057b213-d502-4a5b-9e5d-dbe528264acd",
       "rows": [
        [
         "2685",
         "2685",
         "PBQUS_0001",
         "QUEIMADAS",
         "83",
         "0.362",
         "0.199",
         "0.538",
         "25",
         "10.950000000000001"
        ],
        [
         "2686",
         "2686",
         "PBQUS_0002",
         "QUEIMADAS",
         "83",
         "0.792",
         "0.717",
         "0.865",
         "108",
         "0.8640000000000008"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_idx</th>\n",
       "      <th>ENDERECO_ID</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>ANF</th>\n",
       "      <th>mean</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "      <th>TESTES_ECQ</th>\n",
       "      <th>impacto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>2685</td>\n",
       "      <td>PBQUS_0001</td>\n",
       "      <td>QUEIMADAS</td>\n",
       "      <td>83</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.538</td>\n",
       "      <td>25</td>\n",
       "      <td>10.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>2686</td>\n",
       "      <td>PBQUS_0002</td>\n",
       "      <td>QUEIMADAS</td>\n",
       "      <td>83</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.865</td>\n",
       "      <td>108</td>\n",
       "      <td>0.864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      site_idx ENDERECO_ID  MUNICIPIO  ANF   mean  hdi_2.5%  hdi_97.5%  \\\n",
       "2685      2685  PBQUS_0001  QUEIMADAS   83  0.362     0.199      0.538   \n",
       "2686      2686  PBQUS_0002  QUEIMADAS   83  0.792     0.717      0.865   \n",
       "\n",
       "      TESTES_ECQ  impacto  \n",
       "2685          25   10.950  \n",
       "2686         108    0.864  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sites_cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'output_data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#salve os dataframes ANF, MUNICIPIO e SITE em um arquivo excel unico, colocando sheets para cada um\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Save the dataframes to a single Excel file with separate sheets\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_data.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m      4\u001b[0m     df_anf\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANF\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m     df_mun\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMUNICIPIO\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\F8058552\\AppData\\Local\\anaconda3\\envs\\clusters\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:61\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_sheet_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_sheet_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# the file and later write to it\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode:  \u001b[38;5;66;03m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\F8058552\\AppData\\Local\\anaconda3\\envs\\clusters\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1246\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[0;32m   1243\u001b[0m     cast(IO[\u001b[38;5;28mbytes\u001b[39m], path), compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m   1244\u001b[0m )\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\F8058552\\AppData\\Local\\anaconda3\\envs\\clusters\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'output_data.xlsx'"
     ]
    }
   ],
   "source": [
    "#salve os dataframes ANF, MUNICIPIO e SITE em um arquivo excel unico, colocando sheets para cada um\n",
    "# Save the dataframes to a single Excel file with separate sheets\n",
    "with pd.ExcelWriter(\"output_data.xlsx\") as writer:\n",
    "    df_anf.to_excel(writer, sheet_name=\"ANF\", index=False)\n",
    "    df_mun.to_excel(writer, sheet_name=\"MUNICIPIO\", index=False)\n",
    "    df_site.to_excel(writer, sheet_name=\"SITE\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clusters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
